{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0\n!pip install accelerate peft bitsandbytes transformers trl evaluate sacrebleu sentencepiece wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-26T01:09:50.676444Z","iopub.execute_input":"2024-08-26T01:09:50.676706Z","iopub.status.idle":"2024-08-26T01:10:27.220239Z","shell.execute_reply.started":"2024-08-26T01:09:50.676675Z","shell.execute_reply":"2024-08-26T01:10:27.219151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig","metadata":{"execution":{"iopub.status.busy":"2024-08-26T01:10:27.222085Z","iopub.execute_input":"2024-08-26T01:10:27.222447Z","iopub.status.idle":"2024-08-26T01:10:33.101315Z","shell.execute_reply.started":"2024-08-26T01:10:27.222409Z","shell.execute_reply":"2024-08-26T01:10:33.100541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token=\"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\")","metadata":{"execution":{"iopub.status.busy":"2024-08-26T01:10:33.102381Z","iopub.execute_input":"2024-08-26T01:10:33.102825Z","iopub.status.idle":"2024-08-26T01:10:33.245127Z","shell.execute_reply.started":"2024-08-26T01:10:33.102790Z","shell.execute_reply":"2024-08-26T01:10:33.244305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nconfig = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    trust_remote_code=True,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    trust_remote_code=True,\n    quantization_config=config,\n    device_map=\"auto\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T01:10:33.246978Z","iopub.execute_input":"2024-08-26T01:10:33.247300Z","iopub.status.idle":"2024-08-26T01:12:41.216895Z","shell.execute_reply.started":"2024-08-26T01:10:33.247260Z","shell.execute_reply":"2024-08-26T01:12:41.216004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"openai/gsm8k\", \"main\")","metadata":{"execution":{"iopub.status.busy":"2024-08-26T01:12:41.217961Z","iopub.execute_input":"2024-08-26T01:12:41.218277Z","iopub.status.idle":"2024-08-26T01:12:45.346289Z","shell.execute_reply.started":"2024-08-26T01:12:41.218217Z","shell.execute_reply":"2024-08-26T01:12:45.345390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.DataFrame(ds['train'])\ntest_df = pd.DataFrame(ds['test'])\n\ntest_df","metadata":{"execution":{"iopub.status.busy":"2024-08-26T01:12:45.347380Z","iopub.execute_input":"2024-08-26T01:12:45.347796Z","iopub.status.idle":"2024-08-26T01:12:45.703162Z","shell.execute_reply.started":"2024-08-26T01:12:45.347763Z","shell.execute_reply":"2024-08-26T01:12:45.702284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\ndef get_val(text):\n    after_split = text.strip().split(' ')\n    i = -1\n    while True:\n        final_number = re.sub(r'[^\\d]', '', after_split[i])\n        if final_number != '':\n            break\n        i-=1\n    \n    return final_number\n\ntext = 'mob #### $18,26 mob'\nprint(get_val(text))","metadata":{"execution":{"iopub.status.busy":"2024-08-26T01:35:12.223365Z","iopub.execute_input":"2024-08-26T01:35:12.224226Z","iopub.status.idle":"2024-08-26T01:35:12.230395Z","shell.execute_reply.started":"2024-08-26T01:35:12.224188Z","shell.execute_reply":"2024-08-26T01:35:12.229465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_example(df, idx, include_answer=True):\n    prompt = df.iloc[idx]['question'] + '\\n'\n    if include_answer:\n        answer = df.iloc[idx]['answer']\n        prompt += answer\n    return prompt\n\ndef gen_prompt(df, idx):\n    sys_prompt = \"You are a helpful assistant!\"\n    # 5-shot prompting\n    prompt = \"The following are 5 example math questions. Follow the instructions from these examples to answer the final question:\\n\"\n    for i in range(5):\n        prompt += format_example(train_df, i)\n        prompt += \"\\n\"\n    prompt += \"Now answer this following question:\\n\"\n    prompt += format_example(df, idx, include_answer=False)\n#     print(prompt)\n    \n    messages = [\n      {\"role\": \"system\", \"content\": sys_prompt},\n      {\"role\": \"user\", \"content\": prompt}\n    ]\n\n    return messages\n\n# gen_prompt(test_df, 0)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T01:35:16.419581Z","iopub.execute_input":"2024-08-26T01:35:16.419944Z","iopub.status.idle":"2024-08-26T01:35:16.427049Z","shell.execute_reply.started":"2024-08-26T01:35:16.419908Z","shell.execute_reply":"2024-08-26T01:35:16.426068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef evaluate(df):\n    predicts = []\n    labels = []\n    corr = []\n    for i in tqdm(range(df.shape[0])):\n        message = gen_prompt(df, i)\n        \n        input_ids = tokenizer.apply_chat_template(\n            message,\n            add_generation_prompt=True,\n            return_tensors=\"pt\"\n        ).to(model.device)\n        \n        terminators = [\n            tokenizer.eos_token_id,\n            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n        ]\n\n        outputs = model.generate(\n            input_ids,\n            max_new_tokens=512,\n            eos_token_id=terminators,\n            do_sample=True,\n            temperature=0.1,\n            top_p=0.1,\n        )\n        response = outputs[0][input_ids.shape[-1]:]\n        \n        answer = tokenizer.decode(response, skip_special_tokens=True)\n        \n        predict = get_val(answer)\n        label = get_val(df.iloc[i]['answer'])\n        predicts.append(predict)\n        labels.append(label)\n        \n        cor = (predict == label)\n        corr.append(cor)\n    \n    accuracy = np.mean(corr)\n        \n    return accuracy\n\nevaluate(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T01:35:18.661540Z","iopub.execute_input":"2024-08-26T01:35:18.662426Z","iopub.status.idle":"2024-08-26T01:36:22.566973Z","shell.execute_reply.started":"2024-08-26T01:35:18.662385Z","shell.execute_reply":"2024-08-26T01:36:22.566010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}